# Pic16AProj
Project for Pic16A, Winter 2022 UCLA

Project Name: Twitter Sentiment Analyzer
Group Members: Tyler Nguyen, Max Potter, Brandon Achugbue

Short Description:
Using the NLTK Sentiment Analyzer and the Twitter API, this project will collect and analyze
user sentiments about various topics from their twitter posts. From these tweets, we will predict which tweets are more popular. 
Data will be further processed and analyzed using Scikit-learn to construct models and visualizations of these sentiments, and we specifically used 
the Decision Tree Classifier, lasso models, and Naive Bayes models. 

Instructions For Package Installation:
Packages in the file requirements.txt should be installed to your IDE or device as normal.
Depending on software used, typing:
    pip install <package name here>
        OR
    conda install <package name here>
 Should be more than enough for all packages to function properly. Ensure your IDE has access to each of these.
 For best performance, ensure your matplotlib version is 3.5 or newer.
 Ensure your sklearn version is the newest possible for best results. 

Detailed Description of Demo File:
  The demo file walks through each step itself through markdown cells, however I will go over a brief synopsis here.
  The file defines our standalone function, show_Tree(), which creates a visualization of decision trees like we did in class.
  The demo begins by creating a large twitter api call with the query 'wordle'. 
  It then walks through the steps that should be taken to prepare this data for analysis.
  First, the json should be loaded into the tweetCleaner class.
  This class contains 4 functions that prepare the tweet data to be ready for processing, which include:
    Lowering text to lowercase
    Removing links
    Removing Stopwords
    Removing repeated tweets
  All of these functions can be performed by calling the utility function prepTweets() as a member of the tweetCleaner class.
  We then load the modelAnalyzer class. This should be initiated with a list of all json objects you want to load, but it
  is perfectly fine to leave blank and update json object later.
  This is demonstrated by adding the newly cleaned 'wordle' json object to the model.
  We then move on and add 3 more datasets that  are preloaded into the data, 'Bob_Ross', 'Kanye_Pete', and 'Ukraine_Russia'
  These datasets were generated by the same process, but done beforehand as to not exceed twitter api Rate Limits during testing.
  
  We then call the fitAllData() function. This function goes through each dataset loaded into the model, and fits the data
  to a Decision Tree, a lasso model, and a Native Bayes model. The predictor variables are always 'sentiment' provided by the 
  NLTK sentiment analyzer on the tweets, and the 'bag of words' count_vectorizer of the tweet text. The target variable is the
  number of likes the tweet received.
  
  The performance of each model for each dataset is plotted on a barchart and returned, which is shown in this section. 
  This barchart shows the score of each model, but scores lower than -1 are all automatically shortened to be just -1. This is    because the lasso model in particular is very bad at this kind of analysis, and as such sometimes reaches ridiculously low values that make the chart unreadable.
  
  While the fitAllData() is a great function, you can individually call any model on any dataset, as shown in the next section.
  This is done by first extracting the desired data by calling modelAnalysis.getAllData(). This gets a dictionary
  where each key is a json file name, and the values are the prepared X, y values. 
  We extract these values and then call modelAnalysis.best_fit_tree(), which finds the best depth tree and additionally
  returns a figure that graphs the performance of each different depth of tree.
  This is shown in the figure "Bob Ross Dataset"
  
  Finally, we call the show_Tree function, which gives us a visualization of the created tree.
  
  At the end, we show two examples of exception handling. The first involves the error where we attempt to
  analyze a json file that does not exist. The second involves accidentally exceeding the twitter rate limit. 
  
  
...

Scope and Limitations: There were a few limitations to our project. Firstly, the main limitation was the infinitely large amounts of confounding variables. Popularity of tweets is based on many factors, such as follower count. We could not take all these variables into account, therefore our model did very poorly. In addition, another limitation was the NLTK sentiment analyzer is innately biased. This is because this model is a form of supervised learning, as the words are first classified into positive and negative sentiments by a human. For instance, some words may be  considered negative while other words may be considered positive. Additionally, because we used a term-document matrix in order to classify Tweets as either positive or negative, irony and sarcasm was hard to pick up on. For these reasons, the sentiment score may not accurately reflect what the author of the tweet truly meant as the tone was not picked up on. 
...

License and Terms of Use:
MIT License

References and Acknowledgement: 
- COMM 155 - Artificial Intelligence and New Media, Winter Quarter 2022, Prof. J. Joo, TA. Jacqueline Lam
  - Copy_of_COMM155_W22_Week_5_CSV_format_and_NLTK.ipynb
- https://developer.twitter.com/en/docs/tutorials/step-by-step-guide-to-making-your-first-request-to-the-twitter-api-v2

...

Background Source of Dataset:
  Datasets were all created and loaded using functions in this project from the twitter API. All datasets were updated
  March 17, 2022. 
...

Links to Tutorials:
- Online tutorials involved:
    - Text Analytics for Beginners using NLTK https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk
    - Comparing Sentiment Analysis Tools https://investigate.ai/investigating-sentiment-analysis/comparing-sentiment-analysis-tools/#:~:text=What%27s%20this%20mean%20for%20me%3F%20%23%20%20,words%20%20%20automatic%20based%20on%20score%20
    - Sentiment Analysis Using NLTK https://medium.com/analytics-vidhya/sentiment-analysis-using-nltk-d520f043fc0

...


